{"cells":[{"metadata":{},"cell_type":"markdown","source":"iMaterialist (Fashion) 2019 is a fine-grained  segmentation task, where challengers need to develop algorithms to accurately assign segmentations and attribute labels for fashion images. [Description of the competition](https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6/overview).\n\nThis notebook is currently in development.\n\nThe first step is to implement a baseline model using U-Net. \n\nThe EDA part of this kernel is inspired by [GoldFish](https://www.kaggle.com/go1dfish/updated4-29-fgvc6-simple-eda)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\npd.set_option(\"display.max_rows\", 50)\nimport os\nprint(os.listdir(\"../input\"))\nimport cv2\nimport json\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams[\"font.size\"] = 15\nimport seaborn as sns\nfrom collections import Counter\nfrom PIL import Image\nimport math\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import backend as k","execution_count":1,"outputs":[{"output_type":"stream","text":"['train', 'label_descriptions.json', 'train.csv', 'sample_submission.csv', 'test']\n","name":"stdout"},{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dir = \"../input/\"\ntrain_df = pd.read_csv(input_dir + \"train.csv\")","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Utility functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def json2df(data):\n    df = pd.DataFrame()\n    for idx, el in enumerate(data):\n        for key, val in el.items():\n            df.loc[idx,key] = val\n    return df","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(input_dir + 'label_descriptions.json') as f:\n    label_description = json.load(f)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(json.dumps(label_description['info'],indent = 2))","execution_count":5,"outputs":[{"output_type":"stream","text":"{\n  \"year\": 2019,\n  \"version\": \"1.0\",\n  \"description\": \"The 2019 FGVC^6 iMaterialist Competition - Fashion track dataset.\",\n  \"contributor\": \"iMaterialist Fashion Competition group\",\n  \"url\": \"https://github.com/visipedia/imat_comp\",\n  \"date_created\": \"2019-04-19 12:38:27.493919\"\n}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_df = json2df(label_description[\"categories\"])\nattributes_df = json2df(label_description[\"attributes\"])\ncategory_df['id'] = category_df['id'].astype(int)\ncategory_df['level'] = category_df['level'].astype(int)\nattributes_df['id'] = attributes_df['id'].astype(int)\nattributes_df['level'] = attributes_df['level'].astype(int)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_df.shape[0]","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"46"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"attributes_df.head()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"   id                     name supercategory  level\n0   0   above-the-hip (length)        length      1\n1   1             hip (length)        length      1\n2   2           micro (length)        length      1\n3   3            mini (length)        length      1\n4   4  above-the-knee (length)        length      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>supercategory</th>\n      <th>level</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>above-the-hip (length)</td>\n      <td>length</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>hip (length)</td>\n      <td>length</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>micro (length)</td>\n      <td>length</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>mini (length)</td>\n      <td>length</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>above-the-knee (length)</td>\n      <td>length</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_df.id.count()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"46"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_df.describe()","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"              id  level\ncount  46.000000   46.0\nmean   22.500000    2.0\nstd    13.422618    0.0\nmin     0.000000    2.0\n25%    11.250000    2.0\n50%    22.500000    2.0\n75%    33.750000    2.0\nmax    45.000000    2.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>level</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>46.000000</td>\n      <td>46.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>22.500000</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>13.422618</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>11.250000</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>22.500000</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>33.750000</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>45.000000</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of labels in each image\nlabels_count = train_df.groupby('ImageId')['ClassId'].count().value_counts().sort_index()","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(labels_count.index[:10],labels_count[:10])","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7fa7dc036be0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_train = len(os.listdir('../input/train'))\nprint('The number of training image is {}'.format(num_train))\nnum_test = len(os.listdir('../input/test'))\nprint('The number of testing image is {}'.format(num_test))","execution_count":13,"outputs":[{"output_type":"stream","text":"The number of training image is 45625\nThe number of testing image is 3200\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['ClassId'].unique()","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"array(['6', '0', '28', ..., '10_7_20_24_60_61_88',\n       '10_3_14_19_27_60_61_91', '3_6_11_19_40_60_61_88'], dtype=object)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"WIDTH = 512\nHEIGHT = 512\ntrain_img_dir = \"../input/train/\"","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_num = category_df.shape[0] + 1 \n# Q : why + 1 here??","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploring the image segmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"pallete =  [\n    'Pastel1', 'Pastel2', 'Paired', 'Accent', 'Dark2',\n    'Set1', 'Set2', 'Set3', 'tab10', 'tab20', 'tab20b', 'tab20c']\n\n\ndef make_mask_img(segment_df):\n    seg_width = segment_df.at[0, \"Width\"]\n    seg_height = segment_df.at[0, \"Height\"]\n    seg_img = np.full(seg_width*seg_height, category_num-1, dtype=np.int32)\n    for encoded_pixels, class_id in zip(segment_df[\"EncodedPixels\"].values, segment_df[\"ClassId\"].values):\n        pixel_list = list(map(int, encoded_pixels.split(\" \")))\n        for i in range(0, len(pixel_list), 2):\n            start_index = pixel_list[i] - 1\n            index_len = pixel_list[i+1] - 1\n            # assign the class label\n            seg_img[start_index:start_index+index_len] = int(class_id.split(\"_\")[0])\n    seg_img = seg_img.reshape((seg_height, seg_width), order='F')\n    seg_img = cv2.resize(seg_img, (WIDTH, HEIGHT), interpolation=cv2.INTER_NEAREST)\n\n    return seg_img\n\ndef train_generator(df, batch_size):\n    img_ind_num = df.groupby(\"ImageId\")[\"ClassId\"].count()\n    index = df.index.values[0]\n    trn_images = []\n    seg_images = []\n    for i, (img_name, ind_num) in enumerate(img_ind_num.items()):\n        img = cv2.imread(train_img_dir + img_name)\n        img = cv2.resize(img, (WIDTH, HEIGHT), interpolation=cv2.INTER_AREA)\n        segment_df = (df.loc[index:index+ind_num-1, :]).reset_index(drop=True)\n        index += ind_num\n        if segment_df[\"ImageId\"].nunique() != 1:\n            raise Exception(\"Index Range Error\")\n        seg_img = make_mask_img(segment_df)\n        \n        #         # extra one hot step\n        seg_img = keras.utils.to_categorical(seg_img, num_classes=category_num, dtype=np.int32)\n        \n        # HWC -> CHW\n#         img = img.transpose((2, 0, 1))\n        #seg_img = seg_img.transpose((2, 0, 1))\n        \n        trn_images.append(img)\n        seg_images.append(seg_img)\n        if((i+1) % batch_size == 0):\n            yield np.array(trn_images, dtype=np.float32) / 255, np.array(seg_images, dtype=np.int32)\n            trn_images = []\n            seg_images = []\n    if(len(trn_images) != 0):\n        yield np.array(trn_images, dtype=np.float32) / 255, np.array(seg_images, dtype=np.int32)\n        \ndef cv2plt(img, isColor=True):\n    \"\"\"\n    changing the representation of image from cv2 format to matplotlib format\n    \"\"\"\n    original_img = img\n    original_img = original_img.transpose(1, 2, 0) #order of channel\n    original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB) # image colorspace\n    return original_img\n","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# category_num\n\n# np.unique(segmented[0])\n\n# print(test[1])\n\n# plt.imshow(test[:,:,46], cmap='tab20_r', alpha=0.6)\n\n# plt.imshow(segmented[0], cmap='tab20_r', alpha=0.6)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EDA = False\n\nif EDA==True:\n    original, segmented = train_generator(train_df, 6)\n    fig, ax = plt.subplots(3, 2, figsize=(16, 18))\n    for i, (img, seg) in enumerate(zip(original, segmented)):\n        ax[i//2, i%2].imshow(cv2plt(img))\n        seg[seg == 45] = 255\n        ax[i//2, i%2].imshow(seg, cmap='tab20_r', alpha=0.6)\n        ax[i//2, i%2].set_title(\"Sample {}\".format(i))","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nTo do: Image Augmentation\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def img_gen_trn(df,batch_size):\n    \n    id_num_mask = df.groupby('ImageId').ClassId.count()\n    batch_img,batch_mask = [],[]\n    index = df.index.values[0] #initial index (not exactly zero)\n    for i,(img_id,num_mask) in enumerate(id_num_mask.items()): # i,(img_id,num_mask) ?\n        img = cv2.imread(train_img_dir+img_id)\n        img = cv2.resize(img, (WIDTH, HEIGHT), interpolation=cv2.INTER_AREA)\n        segment_df = train_df.loc[index:index+num_mask-1].reset_index(drop = True)\n        index += num_mask\n        if segment_df['ImageId'].nunique()!= 1:\n            raise Exception(\"Image Index Range Error\")\n        seg_img = make_mask_img(segment_df)\n\n#         # extra one hot step\n        seg_img = keras.utils.to_categorical(seg_img, num_classes=category_num, dtype=np.int32)\n        \n        # HWC -> CHW\n#         img = img.transpose((2,0,1))\n#         print(img.shape)\n        batch_img.append(img)\n        batch_mask.append(seg_img)\n        if((i+1) % batch_size == 0):\n            #yield image if it reaches the batch size\n            yield np.array(batch_img,dtype = np.float32)/255, np.array(batch_mask,dtype = np.int32)\n            batch_img,batch_mask = [],[]\n    if len(batch_img) != 0:\n        yield np.array(batch_img,dtype = np.float32)/255, np.array(batch_mask,dtype = np.int32)","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define the UNET Architecture\n\n[UNET Website](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)\n\n[UNET Paper](https://arxiv.org/abs/1505.04597)\n\n![unet_arch](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)"},{"metadata":{},"cell_type":"markdown","source":"The UNET consists of three path:\n1. Downsampling path\n2. Upsampling path\n3. Skip-connection path\n\nA Down-sampling unit consists of:\n(3x3 convolution layer, ReLU)\n(3x3 convolution layer, ReLU)\n(2x2 max pooling layer with stride of 2)\n\nA Up-sampling unit consists of:\n(3x3 convolution layer, ReLU)\n(3x3 convolution layer, ReLU)\n(2x2 up convolution layer)\n\nThe up convolution layer has many other names, like decovolution layer, or the more acurate one: transposed convolution layer. In `keras` implementations, I haven't figured out whether `UpSampling2D()` layer and `Conv2DTranspose()` layer actually does the same thing, I will return to this matter latter.\n\nBy the way, I have found a very interesting repo done by [vdumoulin](https://github.com/vdumoulin/conv_arithmetic), containing some visualisations of convolution layer. if you find it hard to understand it like me, it might be useful."},{"metadata":{"trusted":true},"cell_type":"code","source":"def down_block(x,filters,kernel_size = (3,3),padding = 'same',strides = 1):\n    c = keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation = \"relu\")(x)\n    c = keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation = \"relu\")(c)\n    p = keras.layers.MaxPool2D(pool_size = (2,2),strides = (2,2))(c)\n    return c,p\n\ndef up_block(x,skip,filters,kernel_size = (3,3),padding = 'same',strides = 1):\n    up = keras.layers.Conv2DTranspose(filters,kernel_size = (2,2),strides = (2,2),padding = 'same')(x)\n    concat = keras.layers.Concatenate()([up,skip])\n    c = keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation = \"relu\")(concat)\n    c = keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation = \"relu\")(c)\n    return c\n\ndef bottle_neck_block(x,filters,kernel_size = (3,3),padding = 'same',strides = 1):\n    c = keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation = \"relu\")(x)\n    c = keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation = \"relu\")(c)\n    return c\n","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def UNet(f = [64,128,256,512,1024]):\n    \n    #input layer\n    inputs = keras.layers.Input(shape = (HEIGHT,WIDTH,3))\n    \n    # downsampling path\n    c1,p1 = down_block(inputs,f[0])\n    c2,p2 = down_block(p1,f[1])\n    c3,p3 = down_block(p2,f[2])\n    c4,p4 = down_block(p3,f[3])\n    \n    # bottle neck path\n    b5 = bottle_neck_block(p4,f[4])\n    \n    # upsampling path\n    u6 = up_block(b5,c4,f[3])\n    u7 = up_block(u6,c3,f[2])\n    u8 = up_block(u7,c2,f[1])\n    u9 = up_block(u8,c1,f[0])\n    \n    #output layer\n#     outputs = keras.layers.Conv2D(category_num,(1,1),activation = 'sigmoid')(u9)\n    outputs = keras.layers.Conv2D(category_num,(1,1),activation = 'softmax',padding = 'same')(u9)\n#     outputs = keras.layers.Conv2D(1,(1,1),activation = 'sigmoid')(u9)\n    model = keras.models.Model(inputs,outputs)\n    return model\n    \n    ","execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dice coefficient from [Mike Clark ](https://gist.github.com/wassname/7793e2058c5c9dacb5212c0ac0b18a8a)\n\nI would recommend this [kernel](https://www.kaggle.com/stkbailey/step-by-step-explanation-of-scoring-metric) if you have troubles understanding the IOU metric. And the credit of the following implementation of `mean_iou` goes to CPMP in the comments in this [kernel](https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice_coef(y_true, y_pred, smooth=1):\n    \"\"\"\n    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n    \"\"\"\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1-dice_coef(y_true, y_pred)\n\n# Define IoU metric\ndef mean_iou(y_true, y_pred):\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        y_pred_ = tf.to_int32(y_pred > t)\n        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2, y_true)\n        k.get_session().run(tf.local_variables_initializer())\n        with tf.control_dependencies([up_opt]):\n            score = tf.identity(score)\n        prec.append(score)\n    return k.mean(k.stack(prec), axis=0)","execution_count":24,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model hyperparameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"FAST_MODE = True\n\nif FAST_MODE == True:\n    WIDTH = 256\n    HEIGHT = 256\n    filter_set = [32,64,128,256,512]\nelse:\n    WIDTH = 512\n    HEIGHT = 512\n    filter_set = [64,128,256,512,1024]    \n    ","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = UNet(filter_set)\nmodel.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['acc'])\nmodel.summary()","execution_count":70,"outputs":[{"output_type":"stream","text":"__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_4 (InputLayer)            (None, 256, 256, 3)  0                                            \n__________________________________________________________________________________________________\nconv2d_57 (Conv2D)              (None, 256, 256, 32) 896         input_4[0][0]                    \n__________________________________________________________________________________________________\nconv2d_58 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_57[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_12 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_58[0][0]                  \n__________________________________________________________________________________________________\nconv2d_59 (Conv2D)              (None, 128, 128, 64) 18496       max_pooling2d_12[0][0]           \n__________________________________________________________________________________________________\nconv2d_60 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_59[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_13 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_60[0][0]                  \n__________________________________________________________________________________________________\nconv2d_61 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_13[0][0]           \n__________________________________________________________________________________________________\nconv2d_62 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_61[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_14 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_62[0][0]                  \n__________________________________________________________________________________________________\nconv2d_63 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_14[0][0]           \n__________________________________________________________________________________________________\nconv2d_64 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_63[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_15 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_64[0][0]                  \n__________________________________________________________________________________________________\nconv2d_65 (Conv2D)              (None, 16, 16, 512)  1180160     max_pooling2d_15[0][0]           \n__________________________________________________________________________________________________\nconv2d_66 (Conv2D)              (None, 16, 16, 512)  2359808     conv2d_65[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_12 (Conv2DTran (None, 32, 32, 256)  524544      conv2d_66[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_12 (Concatenate)    (None, 32, 32, 512)  0           conv2d_transpose_12[0][0]        \n                                                                 conv2d_64[0][0]                  \n__________________________________________________________________________________________________\nconv2d_67 (Conv2D)              (None, 32, 32, 256)  1179904     concatenate_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_68 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_67[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_13 (Conv2DTran (None, 64, 64, 128)  131200      conv2d_68[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_13 (Concatenate)    (None, 64, 64, 256)  0           conv2d_transpose_13[0][0]        \n                                                                 conv2d_62[0][0]                  \n__________________________________________________________________________________________________\nconv2d_69 (Conv2D)              (None, 64, 64, 128)  295040      concatenate_13[0][0]             \n__________________________________________________________________________________________________\nconv2d_70 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_69[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_14 (Conv2DTran (None, 128, 128, 64) 32832       conv2d_70[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_14 (Concatenate)    (None, 128, 128, 128 0           conv2d_transpose_14[0][0]        \n                                                                 conv2d_60[0][0]                  \n__________________________________________________________________________________________________\nconv2d_71 (Conv2D)              (None, 128, 128, 64) 73792       concatenate_14[0][0]             \n__________________________________________________________________________________________________\nconv2d_72 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_71[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_15 (Conv2DTran (None, 256, 256, 32) 8224        conv2d_72[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_15 (Concatenate)    (None, 256, 256, 64) 0           conv2d_transpose_15[0][0]        \n                                                                 conv2d_58[0][0]                  \n__________________________________________________________________________________________________\nconv2d_73 (Conv2D)              (None, 256, 256, 32) 18464       concatenate_15[0][0]             \n__________________________________________________________________________________________________\nconv2d_74 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_73[0][0]                  \n__________________________________________________________________________________________________\nconv2d_75 (Conv2D)              (None, 256, 256, 47) 1551        conv2d_74[0][0]                  \n==================================================================================================\nTotal params: 7,761,615\nTrainable params: 7,761,615\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample training, \ndf_img_id = train_df.groupby('ImageId').ClassId.count()\nprint('number of image in the training dataset: ', df_img_id.shape[0])","execution_count":61,"outputs":[{"output_type":"stream","text":"number of image in the training dataset:  45625\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_train = 5000 #this is the number of (image, label) pair, no the actual number of image.\nnum_val = 500\nbatch_size = 12\nsample_train = train_df.loc[:num_train-1,:]\nsample_val = train_df.loc[num_train:num_train+num_val-1,:]","execution_count":65,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_steps = sample_train.groupby('ImageId').ClassId.count().shape[0]//batch_size\nvalidation_steps = sample_val.groupby('ImageId').ClassId.count().shape[0]//batch_size\ntraining_steps,validation_steps","execution_count":66,"outputs":[{"output_type":"execute_result","execution_count":66,"data":{"text/plain":"(57, 5)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 2\nfor epoch in range(epochs):\n    model.fit_generator(img_gen_trn(sample_train,batch_size=batch_size),\n                        steps_per_epoch=training_steps,\n                        validation_data = img_gen_trn(sample_val,batch_size=batch_size),\n                        validation_steps = validation_steps,\n                        verbose=1,epochs = 1)","execution_count":75,"outputs":[{"output_type":"stream","text":"(687,)\n56/57 [============================>.] - ETA: 2s - loss: 41.4783 - acc: 0.7856(64,)\n5/5 [==============================] - 15s 3s/step - loss: 37.3519 - acc: 0.8069\n57/57 [==============================] - 152s 3s/step - loss: 41.5986 - acc: 0.7849 - val_loss: 37.3519 - val_acc: 0.8069\n(687,)\n56/57 [============================>.] - ETA: 2s - loss: 41.4783 - acc: 0.7856(64,)\n5/5 [==============================] - 15s 3s/step - loss: 37.3519 - acc: 0.8069\n57/57 [==============================] - 150s 3s/step - loss: 41.5986 - acc: 0.7849 - val_loss: 37.3519 - val_acc: 0.8069\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"To do:\n\nfind an implementation of IOU, or DIY"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}