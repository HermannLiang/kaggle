{"cells":[{"metadata":{},"cell_type":"markdown","source":"Like many others, Titanic is the first kaggle completition I attended. In this notebook you will find how I use model stacking, a common ensemble technique on Kaggle, to submit my very first result on the LB! "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sklearn\nimport re\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\nfullset = [train, test] \ntrain.columns\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9366efd12c09ec4f571b28c6e72def69569078e"},"cell_type":"code","source":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92db4afd3b7a650800e34fd19685056ec265a882"},"cell_type":"markdown","source":"The following EDA and feature engineering are inspired  by Sina: https://www.kaggle.com/sinakhorami/titanic-best-working-classifier\n"},{"metadata":{"trusted":true,"_uuid":"6591757afacc1f7ff6c8d605f2574cb5eb5f5c6d"},"cell_type":"code","source":"# Feature Engineering\n\n# Let's tackle each feature one by one\n# PassengerId, leave it there?\n# Survived: our target\n# Pclass\ntrain.Pclass.isnull().value_counts() # there are no null values\n# train.Sex.value_counts()\n# train.Sex.isnull().value_counts()\n# test.Sex.isnull().value_counts()\nfor dataset in fullset:\n    # Sex: let's convert Sex into binary variable. Non-binary shouldn't exist back then right?\n    dataset['Sex'] = dataset['Sex'].map({'male': 0, 'female': 1}).astype(int)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bba1be4edf74129d9a35c36a167691ec2441bb2"},"cell_type":"code","source":"# for age we need imputation\nfor dataset in fullset:\n    age_mean = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_imp = np.random.randint(age_mean - age_std,age_mean + age_std, size = age_null_count)\n    \n    dataset['Age'][np.isnan(dataset['Age'])] = age_imp\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset['Categorical_Age'] = pd.qcut(dataset['Age'],4)\n    \nprint(train[['Survived','Categorical_Age']].groupby(['Categorical_Age'],as_index = False).mean())\nfor dataset in fullset:\n    dataset.loc[dataset['Age']<=21.0,'Age'] = 0\n    dataset.loc[(dataset['Age']>21.0)&(dataset['Age']<=28.0),'Age'] = 1\n    dataset.loc[(dataset['Age']>28.0)&(dataset['Age']<=38.0),'Age'] = 2\n    dataset.loc[dataset['Age']>38.0,'Age'] = 4\n    dataset['Age'] = dataset['Age'].astype(int)\n    \nprint(dataset['Age'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"916052364842b6f523023421f95b29743809e62b"},"cell_type":"code","source":"# Create feature FamilySize from sibsp and parch\nfor dataset in fullset:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\nprint (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a038ab467e3cab10d22ee879f63189392561200"},"cell_type":"code","source":"train['Fare'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0c30257c1103a7001ddb1b5db3f8fa3b2b1df86"},"cell_type":"code","source":"# Fare\n\ntest['Fare'].isnull().value_counts() # only one missing value in testing set\n# we can just use mean imputation\ntest['Fare'] = test['Fare'].fillna(test['Fare'].mean())\nfor dataset in fullset:\n    dataset['Categorical_Fare'] = pd.qcut(dataset['Fare'],4)\n    \nprint(train[['Survived','Categorical_Fare']].groupby('Categorical_Fare',as_index = False).mean())\nprint(train['Categorical_Fare'].value_counts())\nprint(test['Categorical_Fare'].value_counts())\n\nfor dataset in fullset:\n    dataset.loc[dataset['Fare']<=7.91,'Fare'] = 0\n    dataset.loc[(dataset['Fare']<=14.454) & (dataset['Fare']>7.91),'Fare'] = 1\n    dataset.loc[(dataset['Fare']<=31.0) & (dataset['Fare']>14.454),'Fare'] = 2\n    dataset.loc[dataset['Fare']>31.0,'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\nprint(train['Fare'].value_counts())\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af5826f1430ca14f5d04002f862fcccfece21992"},"cell_type":"code","source":"# Cabin Number\ntrain.Cabin.isnull().value_counts()\n# perhaps no need for using this feature??","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7eb2b294b25af44b3b5beb1995de0d994153945"},"cell_type":"code","source":"# Embarked\ntrain.Embarked.isnull().value_counts() # 2 missing values\ntest.Embarked.isnull().value_counts()  # no missing values\n# impute using median\ntrain['Embarked'] = train['Embarked'].fillna('S')\n# train.Embarked.value_counts() # 2 missing values\n\n\n# converting\nembarked_mapping = {'C':0,'Q':1,'S':2}\nfor dataset in fullset:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping).astype(int)\n\nprint(train[['Survived','Embarked']].groupby(['Embarked'],as_index = False).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcccab55055f3fd5f2b15ebf6ad429410bf0e19d"},"cell_type":"code","source":"# credit to Sina \ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\nfor dataset in fullset:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n\nprint(pd.crosstab(train['Title'], train['Sex']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89947466a7872c720baba6e367367bbbc14346db"},"cell_type":"code","source":"# Here let's be creative and get some manual feature different from the original\nfor dataset in fullset:\n    dataset['Title'] = dataset['Title'].replace(['Capt','Major','Col'],'Military')\n    dataset['Title'] = dataset['Title'].replace(['Countess','Don','Dona','Jonkheer','Lady','Master','Sir'],'Nobility')\n    dataset['Title'] = dataset['Title'].replace(['Dr','Rev'],'Educated')\n    #common sense replacement\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \nprint(train[['Survived','Title']].groupby(['Title'],as_index = False).mean())\nprint(test['Title'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"becbc07dafd21b16eee75d17056b85ae253a359a"},"cell_type":"code","source":"title_mapping = {'Educated': 0, 'Military':1,'Miss':2,'Mr':3,'Mrs':4,'Nobility':5}\nfor dataset in fullset:\n    dataset['Title'] = dataset['Title'].map(title_mapping).astype(int)\n    \nprint(train['Title'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# preparing training and testing dataset\ntrain_X = train.drop(['Survived','Name','Ticket','Categorical_Age','Categorical_Fare',\n                     'PassengerId','SibSp','Parch','Cabin'],axis = 1)\n# test_X = test.drop(['Name','Ticket'],axis = 1)\ntrain_y = train['Survived']\ntest_X = test.drop(['Name','Ticket','Categorical_Age','Categorical_Fare',\n                     'PassengerId','SibSp','Parch','Cabin'],axis = 1)\n# retain only numpy array\ntrain_X = train_X.values\ntrain_y = train_y.values\ntest_X = test_X.values\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62971bc5c2c057b84a14101c6f75234079f51cc7"},"cell_type":"markdown","source":"Investigate different classifiers:\n* Logistic Regression\n* SVM\n* Decision Tree\n* Random Forest\n* AdaBoost\n* Gradient Boosting Classifier\n* Multilayer perceptron\n* Gaussian Naive Bayes\n* Linear Discriminant Analysis\n* K-Nearest Neighbor\n\n\n"},{"metadata":{"trusted":true,"_uuid":"baf1eeb049cd199d4dcf960f1ef0cabcbc2371c5"},"cell_type":"code","source":"# import model\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score\nSEED = 0\nsss = StratifiedShuffleSplit(n_splits = 10,test_size= 0.1,random_state = SEED)\nsss.split(train_X,train_y)\nclassifiers = [SVC(),\n               RandomForestClassifier(),\n               AdaBoostClassifier(),\n               GradientBoostingClassifier(),\n               LogisticRegression(),\n               GaussianNB(),\n               KNeighborsClassifier(),\n               LinearDiscriminantAnalysis(),\n               MLPClassifier(),\n               DecisionTreeClassifier()]\n\nacc_table = {} # a dictionary store the prediction\nfor train_index, test_index in sss.split(train_X,train_y):\n    train_X_cv, test_X_cv = train_X[train_index],train_X[test_index]\n    train_y_cv, test_y_cv = train_y[train_index],train_y[test_index]\n    for clf in classifiers:\n        name = clf.__class__.__name__\n        clf.fit(train_X_cv,train_y_cv)\n        predict_y = clf.predict(test_X_cv)\n        acc = accuracy_score(test_y_cv,predict_y)\n        if name in acc_table:\n            acc_table[name] += acc\n        else:\n            acc_table[name] = acc\n\nfor name in acc_table:\n    acc_table[name] = acc_table[name]/len(classifiers)\nprint(acc_table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaecc7afab1e0a5c195f04f2b22a58d4884a5a42"},"cell_type":"code","source":"# print()\n# acc_df = pd.DataFrame(acc_table.items(),columns = ['Classifier','Accuracy'])\n\nacc_df = pd.DataFrame(list(acc_table.items()),columns = ['Classifier','Accuracy'])\n# acc_df.index.name = 'Classifier'\n# acc_df.reset_index()\nacc_df = acc_df.sort_values('Accuracy',ascending = 0)\n# acc_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3319b9d7cae99179bdebaacec66313906c1a20bb"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Classifier', data=acc_df, color=\"b\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff58001deb07c5f380c82b0ca072dc1c74c4c617"},"cell_type":"markdown","source":"Here, we can use the top five classifiers as our first stacking layer.\nOr using all of them and see how things go?\nWe can repeat the fit and predict process but this time around we wrap them into pipeline for simplification.\n"},{"metadata":{"trusted":true,"_uuid":"236ed7951f0ea1a026e0aaa452b4515f3291d2db"},"cell_type":"code","source":"# from sklearn.model_selection import KFold\n# SEED = 0\n# kf = KFold(n_splits=5,random_state=SEED)\n# kf.get_n_splits(train_X)\n# # train_idx, test_idx = kf.split(train_X,train_y)\n# type(kf)\n# for i, (trainindex, testindex) in enumerate(kf.split(train_X)): \n#     print(\"%s %s\" % (trainindex, testindex))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9749f96334bb7536e4a6c768887f0e87fc12b11"},"cell_type":"code","source":"NFOLD = 10\nsss = StratifiedShuffleSplit(n_splits = NFOLD,test_size= 0.1,random_state = SEED)\nclass SklearnHelper(object):\n    def __init__(self,clf,seed=0,params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n    def train(self, train_X, train_y):\n        self.clf.fit(train_X, train_y)\n    def predict(self, x):\n        return self.clf.predict(x)\n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n    \ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLD, ntest))\n\n    for i, (train_index, test_index) in enumerate(sss.split(x_train,y_train)):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"380b890f71a227a1e62a2dc245f88a9bcffb630a"},"cell_type":"code","source":"# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'C' : 0.025\n    }\nlogreg_params = {'max_iter' : 100}\nmlp_params = {'verbose' : 0}\ndt_params = {'min_samples_split' : 2}\n\nsvc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)\nrf = SklearnHelper(clf = RandomForestClassifier, seed=SEED,params=rf_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nlogreg = SklearnHelper(clf=LogisticRegression, seed=SEED, params=logreg_params)\nmlp = SklearnHelper(clf=MLPClassifier, seed=SEED, params=mlp_params)\ndt = SklearnHelper(clf=DecisionTreeClassifier, seed=SEED, params=dt_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3aee9217efca3bd4062b88d0400853c05cadc6bd"},"cell_type":"code","source":"x_train = train_X\ny_train = train_y\nx_test = test_X\n\nntrain = x_train.shape[0]\nntest = x_test.shape[0]\ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \nlogreg_oof_train, logreg_oof_test = get_oof(logreg, x_train, y_train, x_test) # logreg\nmlp_oof_train, mlp_oof_test = get_oof(mlp, x_train, y_train, x_test) # mlp\ndt_oof_train, dt_oof_test = get_oof(dt, x_train, y_train, x_test) # mlp\nprint(\"Training is complete\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc4d9d016ceba02692d1c6e8045a898b746d1a16"},"cell_type":"code","source":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n      'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel(),\n      'DecisionTrees': dt_oof_train.ravel(),\n      'MLP': mlp_oof_train.ravel(),\n      'LogReg':logreg_oof_train.ravel(),\n      'SVC':svc_oof_train.ravel()\n                                       })\nbase_predictions_train.head()\n\npy.init_notebook_mode(connected=True)\ndata = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b02092917d684a00ba50b7c5ed49bdcd7fe6989e"},"cell_type":"code","source":"x_train = np.concatenate((rf_oof_train, ada_oof_train, gb_oof_train, dt_oof_train,mlp_oof_train,logreg_oof_train,\n                          svc_oof_train), axis=1)\nx_test = np.concatenate((rf_oof_test, ada_oof_test, gb_oof_test, dt_oof_test,mlp_oof_test,mlp_oof_test,\n                         svc_oof_test), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ef6eb9594d1f82777f19c6e0a6e3787c12b2fd6"},"cell_type":"code","source":"gbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5fed763ddfa96913e6f4227169540dd4688c737"},"cell_type":"code","source":"SubmissionCL = pd.DataFrame({ 'PassengerId': test['PassengerId'],\n                            'Survived': predictions })\nSubmissionCL.to_csv(\"SubmissionCL.csv\", index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}